# -*- coding: utf-8 -*-
"""Copy of Electric Vehicle price prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eWZXR0YgisU-4QikFwl-WLKLJmXBqqhZ

Importing the dependencies
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from scipy.stats import randint, uniform
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, IsolationForest, BaggingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error, accuracy_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from xgboost import callback
from xgboost import XGBRegressor
from xgboost import XGBClassifier
from xgboost.callback import EarlyStopping
from sklearn.datasets import make_regression
from scipy import stats

pip install -U scikit-learn

pip install xgboost

pip install --upgrade xgboost

"""Data Collection and Processing"""

data = pd.read_csv("/content/ElectricCarData_Norm.csv")
print(data)

"""Dimensions of the dataset"""

print("Initial data shape:", data.shape)
print("\nFirst 5 rows:\n", data.head())
print("\nData types:\n", data.dtypes)
print("\nMissing values:\n", data.isnull().sum())

"""****

CHECK DATA TYPES AND USE ONE HOT ENCODING

---
"""

print(data.dtypes)

"""Remove the units in columns and apply one - hot encoding"""

def remove_units(value):
    """Remove units from values and convert to float, handling edge cases."""
    try:
        if pd.isna(value):
            return np.nan
        if isinstance(value, (int, float)):
            return float(value)

        if isinstance(value, str) and '%' in value:
            cleaned = re.sub(r'[^0-9.-]', '', value)
            return float(cleaned) / 100 if cleaned else np.nan

        cleaned = re.sub(r'[^\d.-]', '', str(value))
        return float(cleaned) if cleaned else np.nan
    except (ValueError, TypeError):
        return np.nan

num_cols = ['Accel', 'TopSpeed', 'Range', 'Efficiency', 'FastCharge', 'Seats', 'PriceEuro']
existing_num_cols = [col for col in num_cols if col in data.columns]

for col in existing_num_cols:
    data[col] = data[col].apply(remove_units)

    if col == 'Efficiency' and data[col].max() > 1:
        data[col] = data[col] / 100

if existing_num_cols:
    num_imputer = SimpleImputer(strategy='median')
    data[existing_num_cols] = num_imputer.fit_transform(data[existing_num_cols])

categorical_columns = ['Brand', 'Model', 'RapidCharge', 'PowerTrain', 'PlugType', 'BodyStyle', 'Segment']
existing_cat_cols = [col for col in categorical_columns if col in data.columns]

if existing_cat_cols:
    for col in existing_cat_cols:
        freq_encoding = data[col].value_counts(normalize=True)
        data[col] = data[col].map(freq_encoding)

final_df = pd.concat([
    data[existing_num_cols],
    data[existing_cat_cols]
], axis=1)

print("\nData Summary:")
print(f"Original shape: {data.shape}")
print(f"Processed shape: {final_df.shape}")

print("\nMissing values after processing:")
print(final_df.isna().sum().sum(), "missing values remaining")

print("\nSample of processed numerical columns:")
print(final_df[existing_num_cols].head(3) if existing_num_cols else "No numerical columns processed")

print("\nSample of encoded categorical columns:")
encoded_cat_cols = [col for col in final_df.columns if any(x in col for x in existing_cat_cols)]
print(encoded_cat_cols[:5] if encoded_cat_cols else "No categorical columns encoded")

print("\nFirst 3 rows of final data:")
print(final_df.head(3))

"""**Adjust** **outliers**"""

def adjust_outliers_iqr(df):
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df.clip(lower=lower_bound, upper=upper_bound, axis=1)

"""**Univariate Analysis**"""

def univariate_summary(data):
    summary = pd.DataFrame({
        'Feature': data.columns,
        'Count': data.count(),
        'Missing': data.isnull().sum(),
        'Mean': data.mean(),
        'Median': data.median(),
        'Std Dev': data.std(),
        'Min': data.min(),
        'Max': data.max(),
        'Skewness': data.skew(),
        'Kurtosis': data.kurt()
    }).reset_index(drop=True)
    return summary

"""GridSearchCV"""

def gridsearch_cv(model, param_grid, X_train, y_train, cv=5, scoring='r2', n_jobs=-1):
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        cv=cv,
        scoring=scoring,
        n_jobs=n_jobs
    )

    grid_search.fit(X_train, y_train)

    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_
    best_score = grid_search.best_score_

    return best_model, best_params, best_score

"""**Separating the features and target**"""

def univariate_summary(df):
    summary = pd.DataFrame(index=df.columns)

    summary['Null_Count'] = df.isnull().sum()
    summary['NonNull_Count'] = df.notnull().sum()
    summary['Total_Count'] = df.shape[0]

    summary['Mean'] = df.mean()
    summary['Median'] = df.median()
    summary['Mode'] = df.mode().iloc[0]
    summary['STD'] = df.std()
    summary['Min'] = df.min()

    Q1 = df.quantile(0.25)
    Q2 = df.quantile(0.5)
    Q3 = df.quantile(0.75)
    Q4 = df.quantile(1.0)

    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    summary['Q1:25%'] = Q1
    summary['Q2:50%'] = Q2
    summary['Q3:75%'] = Q3
    summary['Q4:100%'] = Q4
    summary['IQR'] = IQR
    summary['1.5 Rule'] = 1.5 * IQR

    summary['Lesser'] = (df < lower_bound).sum()
    summary['Greater'] = (df > upper_bound).sum()

    return summary.T

df_numeric = data.select_dtypes(include=[np.number])
summary_table = univariate_summary(df_numeric)

print("=== Univariate Summary ===")
print(summary_table)

for col in df_numeric.columns:
    plt.figure(figsize=(8, 4))
    sns.histplot(df_numeric[col], kde=True, bins=30)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.tight_layout()
    plt.show()

summary_table.to_csv("univariate_summary.csv")

pip install ydata-profiling

from ydata_profiling import ProfileReport

df = pd.read_csv("univariate_summary.csv")
profile = ProfileReport(df, title="Univariate Summary", explorative=True)
profile.to_file("summary_report.html")

df_numeric = data.select_dtypes(include=[np.number])

summary_table = univariate_summary(df_numeric)
print("=== Univariate Summary ===")
print(summary_table)

for col in df_numeric.columns:
    plt.figure(figsize=(8, 4))
    sns.histplot(df_numeric[col], kde=True, bins=30)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.tight_layout()
    plt.show()

df_adjusted = adjust_outliers_iqr(final_df)
df_adjusted

arget_col = 'PriceEuro'

if target_col in df_adjusted.columns:
    X = df_adjusted.drop(columns=[target_col]).copy()
    Y = df_adjusted[target_col]

print(X)

print(Y)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)

def plot_box(data, cols, title):
    plt.figure(figsize=(10, 5))
    sns.boxplot(data=data[cols])
    plt.yscale('log')
    plt.title(title)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

def cap_outliers(df, columns):
    df_capped = df.copy()
    for col in columns:
        Q1 = df_capped[col].quantile(0.25)
        Q3 = df_capped[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df_capped[col] = df_capped[col].clip(lower=lower_bound, upper=upper_bound)
    return df_capped

train_df, test_df = train_test_split(df_adjusted, test_size=0.2, random_state=42)

train_columns_capped = cap_outliers(train_df, num_cols)
test_columns_capped = cap_outliers(test_df, num_cols)

def plot_all_boxplots(train_df, train_capped, test_df, test_capped, cols):
    fig, axes = plt.subplots(2, 2, figsize=(16, 10))

    sns.boxplot(data=train_df[cols], ax=axes[0, 0])
    axes[0, 0].set_title("(a) Train Dataset - With Outliers")
    axes[0, 0].set_yscale('log')
    axes[0, 0].tick_params(axis='x', rotation=45)

    sns.boxplot(data=train_capped[cols], ax=axes[0, 1])
    axes[0, 1].set_title("(b) Train Dataset - Adjusted Outliers")
    axes[0, 1].set_yscale('log')
    axes[0, 1].tick_params(axis='x', rotation=45)

    sns.boxplot(data=test_df[cols], ax=axes[1, 0])
    axes[1, 0].set_title("(c) Test Dataset - With Outliers")
    axes[1, 0].set_yscale('log')
    axes[1, 0].tick_params(axis='x', rotation=45)

    sns.boxplot(data=test_capped[cols], ax=axes[1, 1])
    axes[1, 1].set_title("(d) Test Dataset - Adjusted Outliers")
    axes[1, 1].set_yscale('log')
    axes[1, 1].tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

plot_all_boxplots(train_df, train_columns_capped, test_df, test_columns_capped, num_cols)

print(X_train)

print(X.shape, X_train.shape, X_test.shape)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print(X_train_scaled)

"""**MODEL TRAINING**

**LINEAR REGRESSION**
"""

model = LinearRegression()
model.fit(X_train, Y_train)

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

mae = mean_absolute_error(Y_test, y_test_pred)
mse = mean_squared_error(Y_test, y_test_pred)
rmse = np.sqrt(mse)
mape = mean_absolute_percentage_error(Y_test, y_test_pred) * 100
accuracy = 100 - mape
train_r2 = r2_score(Y_train, y_train_pred)
test_r2 = r2_score(Y_test, y_test_pred)

print("\n=== Linear Regression Results ===")
print(f"Training R²: {train_r2:.2f}")
print(f"Testing R²: {test_r2:.2f}")
print(f"Mean Absolute Error: {mae:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"MAPE: {mape:.2f}%")
print(f"Testing Accuracy: {accuracy:.2f}%")

if train_r2 < 0.5 and test_r2 < 0.5:
    print("\nModel is underfitting (Poor performance on all metrics)")
elif abs(train_r2 - test_r2) < 0.05:
    print("\nModel is well-fitted")
elif train_r2 > test_r2 + 0.1:
    print("\nModel is overfitting (Training R² > Test R²)")
else:
    print("\nModel shows mixed behavior")

"""**ORIGINAL GBM**"""

gbm_model = GradientBoostingRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=5,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42
)

gbm_model.set_params(
    n_iter_no_change=10,
    validation_fraction=0.2
)

scores = cross_val_score(gbm_model, X_train_scaled, Y_train,
                        cv=5, scoring='r2', n_jobs=-1)

gbm_model.fit(X_train_scaled, Y_train)

y_train_pred = gbm_model.predict(X_train_scaled)
y_test_pred = gbm_model.predict(X_test_scaled)

metrics = {
    'MAE': mean_absolute_error(Y_test, y_test_pred),
    'MSE': mean_squared_error(Y_test, y_test_pred),
    'RMSE': np.sqrt(mean_squared_error(Y_test, y_test_pred)),
    'R²': r2_score(Y_test, y_test_pred),
    'MAPE': mean_absolute_percentage_error(Y_test, y_test_pred) * 100
}
metrics['Accuracy'] = 100 - metrics['MAPE']
train_r2 = r2_score(Y_train, y_train_pred)
test_r2 = r2_score(Y_test, y_test_pred)

print("==== Gradient Boosting Results ====")
print(f"MAE: {metrics['MAE']:.2f}")
print(f"MSE: {metrics['MSE']:.2f}")
print(f"RMSE: {metrics['RMSE']:.2f}")
print(f"R²: {metrics['R²']:.2f}")
print(f"Accuracy: {metrics['Accuracy']:.2f}%")
print(f"Mean CV R²: {scores.mean():.2f} (±{scores.std():.2f})")

if train_r2 < 0.5 and test_r2 < 0.5:
    print("\nModel is underfitting (Poor performance on all metrics)")
elif abs(train_r2 - test_r2) < 0.05:
    print("\nModel is well-fitted")
elif train_r2 > test_r2 + 0.1:
    print("\nModel is overfitting (Training R² > Test R²)")
else:
    print("\nModel shows mixed behavior")

"""OPTIMIZED GBM"""

X_train_df = pd.DataFrame(X_train_scaled)
y_train_series = pd.Series(Y_train)
model.fit(X_train_scaled, Y_train)

y_pred = model.predict(X_test_scaled)
X_cleaned = adjust_outliers_iqr(X_train_df)
y_cleaned = y_train_series
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7]
}

grid_search = GridSearchCV(
    GradientBoostingRegressor(random_state=42),
    param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1
)

grid_search.fit(X_cleaned, y_cleaned)
best_gbm = grid_search.best_estimator_

y_train_pred = best_gbm.predict(X_cleaned)
train_r2 = r2_score(y_cleaned, y_train_pred)

y_test_pred = best_gbm.predict(X_test_scaled)
metrics = {
    'MAE': mean_absolute_error(Y_test, y_test_pred),
    'MSE': mean_squared_error(Y_test, y_test_pred),
    'RMSE': np.sqrt(mean_squared_error(Y_test, y_test_pred)),
    'MAPE': mean_absolute_percentage_error(Y_test, y_test_pred) * 100
}
metrics['Accuracy'] = 100 - metrics['MAPE']

train_r2 = r2_score(Y_train, y_train_pred)
test_r2 = r2_score(Y_test, y_test_pred)
cv_scores = cross_val_score(best_gbm, X_cleaned, y_cleaned, cv=5, scoring='r2')

print("\n==== Optimized Gradient Boosting Results ====")
print(f"Best Parameters: {grid_search.best_params_}")
for metric, value in metrics.items():
    print(f"{metric}: {value:.2f}" + ("%" if metric in ['MAPE', 'Accuracy'] else ""))
print(f"\nTraining R²: {train_r2:.2f}")
print(f"Testing R²: {test_r2:.2f}")
print(f"Cross-Validation R² Scores: {cv_scores}")
print(f"Mean CV R²: {cv_scores.mean():.2f}")

if train_r2 < 0.5 and test_r2 < 0.5:
    print("\nModel is underfitting (Poor performance on all metrics)")
elif abs(train_r2 - test_r2) < 0.05:
    print("\nModel is well-fitted")
elif train_r2 > test_r2 + 0.1:
    print("\nModel is overfitting (Training R² > Test R²)")
else:
    print("\nModel shows mixed behavior")

"""**RANDOM** **FOREST**"""

rf_model = RandomForestRegressor(n_estimators=100, random_state=0)
rf_model.fit(X_train_scaled, Y_train)

y_pred_rf = rf_model.predict(X_test_scaled)

mae = mean_absolute_error(Y_test, y_pred_rf)
mse = mean_squared_error(Y_test, y_pred_rf)
rmse = np.sqrt(mse)
r2 = r2_score(Y_test, y_pred_rf)
mape = mean_absolute_percentage_error(Y_test, y_pred_rf) * 100
accuracy = 100 - mape

print("==== Random Forest Results ====")
print(f"MAE: {mae:.2f}")
print(f"MSE: {mse:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.2f}")
print(f"Accuracy: {accuracy:.2f}%")

train_r2 = r2_score(Y_train, y_train_pred)
test_r2 = r2_score(Y_test, y_test_pred)

if train_r2 < 0.5 and test_r2 < 0.5:
    print("\nModel is underfitting (Poor performance on all metrics)")
elif abs(train_r2 - test_r2) < 0.05:
    print("\nModel is well-fitted")
elif train_r2 > test_r2 + 0.1:
    print("\nModel is overfitting (Training R² > Test R²)")
else:
    print("\nModel shows mixed behavior")

"""**KNN**"""

!apt-get install libgomp1

param_grid = {'n_neighbors': range(3, 20)}
grid_search = GridSearchCV(KNeighborsRegressor(), param_grid, cv=5, scoring='r2')
grid_search.fit(X_train_scaled, Y_train)

best_k = grid_search.best_params_['n_neighbors']
knn_model = KNeighborsRegressor(n_neighbors=best_k)
knn_model.fit(X_train_scaled, Y_train)

y_pred_knn = knn_model.predict(X_test_scaled)

mae = mean_absolute_error(Y_test, y_pred_knn)
mse = mean_squared_error(Y_test, y_pred_knn)
rmse = np.sqrt(mse)
r2 = r2_score(Y_test, y_pred_knn)
mape = mean_absolute_percentage_error(Y_test, y_pred_knn) * 100
accuracy = 100 - mape

print("==== Optimized KNN Results ====")
print(f"Best k: {best_k}")
print(f"MAE: {mae:.2f}")
print(f"MSE: {mse:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.2f}")
print(f"Accuracy: {accuracy:.2f}%")

train_r2 = r2_score(Y_train, y_train_pred)
test_r2 = r2_score(Y_test, y_test_pred)

if train_r2 < 0.5 and test_r2 < 0.5:
    print("\nModel is underfitting (Poor performance on all metrics)")
elif abs(train_r2 - test_r2) < 0.05:
    print("\nModel is well-fitted")
elif train_r2 > test_r2 + 0.1:
    print("\nModel is overfitting (Training R² > Test R²)")
else:
    print("\nModel shows mixed behavior")

"""**XGBoost**"""

xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=0)
xgb_model.fit(X_train_scaled, Y_train)

y_pred_xgb = xgb_model.predict(X_test_scaled)

mae = mean_absolute_error(Y_test, y_pred_xgb)
mse = mean_squared_error(Y_test, y_pred_xgb)
rmse = np.sqrt(mse)
r2 = r2_score(Y_test, y_pred_xgb)
mape = mean_absolute_percentage_error(Y_test, y_pred_xgb) * 100
accuracy = 100 - mape

print("==== XGBoost Results ====")
print(f"MAE: {mae:.2f}")
print(f"MSE: {mse:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.2f}")
print(f"Accuracy: {accuracy:.2f}%")

train_r2 = r2_score(Y_train, y_train_pred)
test_r2 = r2_score(Y_test, y_test_pred)

if train_r2 < 0.5 and test_r2 < 0.5:
    print("\nModel is underfitting (Poor performance on all metrics)")
elif abs(train_r2 - test_r2) < 0.05:
    print("\nModel is well-fitted")
elif train_r2 > test_r2 + 0.1:
    print("\nModel is overfitting (Training R² > Test R²)")
else:
    print("\nModel shows mixed behavior")

"""**HYBRID MODEL**"""

pip install xgboost lightgbm scikit-learn

!pip install catboost

import warnings
warnings.filterwarnings('ignore')

xgb_model = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=7, subsample=0.8, colsample_bytree=0.8, random_state=42)
gbm_model = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=7, subsample=0.8, random_state=42)
lgb_model = LGBMRegressor(n_estimators=500, learning_rate=0.05, max_depth=7, subsample=0.8, colsample_bytree=0.8, random_state=42)

xgb_model.fit(X_train_scaled, Y_train)
gbm_model.fit(X_train_scaled, Y_train)
lgb_model.fit(X_train_scaled, Y_train)

xgb_pred_train = xgb_model.predict(X_train_scaled)
gbm_pred_train = gbm_model.predict(X_train_scaled)
lgb_pred_train = lgb_model.predict(X_train_scaled)

xgb_pred_test = xgb_model.predict(X_test_scaled)
gbm_pred_test = gbm_model.predict(X_test_scaled)
lgb_pred_test = lgb_model.predict(X_test_scaled)

meta_train = np.column_stack((xgb_pred_train, gbm_pred_train, lgb_pred_train))
meta_test = np.column_stack((xgb_pred_test, gbm_pred_test, lgb_pred_test))

meta_model = XGBRegressor(n_estimators=300, learning_rate=0.03, max_depth=5, random_state=42)
meta_model.fit(meta_train, Y_train)

final_predictions = meta_model.predict(meta_test)

y_true = np.concatenate((Y_train, Y_test), axis=0)
y_pred = np.concatenate((meta_model.predict(meta_train), meta_model.predict(meta_test)), axis=0)

kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X_train_scaled, Y_train, cv=kf, scoring='r2')
print("R² Scores for each fold:", scores)
print("Average R² Score:", np.mean(scores))

total_rmse = mean_squared_error(y_true, y_pred) ** 0.5
total_mae = mean_absolute_error(y_true, y_pred)  # MAE
total_mse = mean_squared_error(y_true, y_pred)  # MSE
total_r2 = r2_score(y_true, y_pred)  # R² Score
total_mape = mean_absolute_percentage_error(y_true, y_pred)  # MAPE

train_acc = meta_model.score(meta_train, Y_train)
test_acc = meta_model.score(meta_test, Y_test)

print("Total RMSE:", total_rmse)
print("Total MAE:", total_mae)
print("Total MSE:", total_mse)
print("Total R² Score:", total_r2)
print("Total MAPE:", total_mape)

accuracy = 100 - (total_mape * 100)
print("Overall Model Accuracy:", accuracy, "%")

train_r2 = r2_score(Y_train, y_train_pred)
test_r2 = r2_score(Y_test, y_test_pred)

print("Training R2: ", train_r2)
print("Testing R2: ", test_r2)

if train_r2 < 0.5 and test_r2 < 0.5:
    print("\nModel is underfitting")
elif abs(train_r2 - test_r2) < 0.05 and train_r2 > 0.5 and test_r2 > 0.5:
    print("\nModel is well-fitted")
elif train_r2 > test_r2 + 0.1:
    print("\nModel is overfitting")
else:
    print("\nModel shows mixed behavior")

